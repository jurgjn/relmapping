#!/usr/bin/env python

import argparse
import collections
import csv
import itertools
import logging
import os
import math
import signal
import sys

from pprint import pprint

import numpy as np
import pandas as pd
import pyBigWig

signal.signal(signal.SIGPIPE, signal.SIG_DFL)
logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s; %(levelname)s; %(funcName)s; %(message)s',
        datefmt='%y-%m-%d %H:%M:%S')

def prepare_second_derivative_kernel(width, times):
    kernel = np.array([-1.0/560, 8.0/315, -1.0/5, 8.0/5, -205.0/72, 8.0/5, -1.0/5, 8.0/315, -1.0/560]) # doi:10.1090/S0025-5718-1988-0935077-0
    rolling_mean_kernel = np.ones(width)/float(width)
    for i in range(times):
        kernel = np.convolve(kernel, rolling_mean_kernel)
    return 1e6*kernel[::-1]

def fill_gap(y):
    y[np.isnan(y)] = 0
    s = np.where(np.diff(y) < 0)[0] + 1
    e = np.where(np.diff(y) > 0)[0] + 1
    if s[0] > e[0]:
        e = e[1:]
    if s[-1] > e[-1]:
        s = s[0:(len(s)-1)]
    for i,j in zip(s,e):
        y[i:j] = (y[i-1]+y[j])/2
    return y

def is_bw(fp):
    with pyBigWig.open(fp) as fh:
        f = fh.isBigWig()
    return f

def coverage(l_fp_inp, fp_out):
    print('calculating mean of all input signal:')
    l_fh_inp = [pyBigWig.open(fp_inp) for fp_inp in l_fp_inp]
    assert pyBigWig.numpy == 1
    assert all([fh_inp.isBigWig() for fh_inp in l_fh_inp])

    fh_out = pyBigWig.open(fp_out, 'w')
    fh_out.addHeader([(chrom, size) for (chrom, size) in l_fh_inp[0].chroms().items()])

    for chrom, size in itertools.islice(l_fh_inp[0].chroms().items(), None):
        l_val = []
        print(chrom, size)
        for fh_inp in l_fh_inp:
            #val = fill_gap(fh_inp.values(chrom, 0, size, numpy=True)) # This is doing something else.
            val = fh_inp.values(chrom, 0, size, numpy=True)
            #print(val.shape)
            l_val.append(val)
        fh_out.addEntries(chrom, 0, values=np.mean(l_val, axis=0), span=1, step=1)

    # Close files
    for fh_inp in l_fh_inp:
        fh_inp.close()
    fh_out.close()

def d2smooth(fp_inp, fp_out, kernel):
    fh_inp = pyBigWig.open(fp_inp) 
    fh_out = pyBigWig.open(fp_out, 'w')
    fh_out.addHeader([(chrom, size) for (chrom, size) in fh_inp.chroms().items()])

    for chrom, size in itertools.islice(fh_inp.chroms().items(), None):
        print(chrom, size)
        val = fh_inp.values(chrom, 0, size, numpy=True)
        val_d2 = np.convolve(val, kernel, mode='same')
        fh_out.addEntries(chrom, 0, values=val_d2, span=1, step=1)

    fh_inp.close()
    fh_out.close()

def write_gffbed(fp,
                 chrom, start, end, name='', attr=None, score=0, strand='.', 
                 thickStart=None, thickEnd=None, itemRgb='#0072b2', 
                 trackline='#track gffTags=on', v=False):
    df = pd.DataFrame()
    def force_iter(l_):
        try:
            l = list(l_)
            if (len(l) > 1) and not(type(l_) is str):
                return l
            else:
                return list(itertools.repeat(l_, len(df)))
        except TypeError:
            return list(itertools.repeat(l_, len(df)))
    #return list(l) if hasattr(l, '__iter__') else list(itertools.repeat(l, len(df)))
    df['chrom'] = list(chrom)
    df['start'] = force_iter(start)
    df['end'] = force_iter(end)
    def pack_row(ir): return (";".join([("%s=%s" % (k, v)).replace(" ", "%20") for k, v in zip(ir[1].index, ir[1])]))
    attr_ = pd.concat([pd.DataFrame({'Name': force_iter(name)}), attr], axis=1)
    df['name'] = list(map(pack_row, attr_.iterrows()))
    df['score'] = force_iter(score)
    df['strand'] = force_iter(strand)

    if not(thickStart is None):
        df['thickStart'] = force_iter(thickStart)       
    else:
        df['thickStart'] = df['start'].copy().tolist()

    if not(thickEnd is None):
        df['thickEnd'] = force_iter(thickEnd)
    else:
        df['thickEnd'] = df['end'].copy().tolist()

    df['itemRgb'] = force_iter(itemRgb)
    with open(fp, 'w') as fh:
        print(trackline, file=fh)
        df.sort_values(['chrom', 'start', 'end']).to_csv(fh, sep='\t', index=False, header=False, quoting=csv.QUOTE_NONE)
    if v: return df

# https://doi.org/10.1101/gr.153668.112
# For analyses where a single TSS position was required, we considered the distribution of cap 5' ends 
# within the TIC, and selected the position with the most tags (the mode). In the case of a tie (two or more 
# positions with the same number of tags), we selected the mode closest to the median of the TIC.
def nanargmax_median(a):
    a_max = np.nanmax(a)
    a_max_indices = np.flatnonzero(a == a_max)
    if len(a_max_indices) == 0:
        return 0
    return a_max_indices[len(a_max_indices) // 2]

assert nanargmax_median([1,2,3,3,3,2,1]) == 3
assert nanargmax_median([1,2,2,1]) == 2
assert nanargmax_median([1]) == 0

def find_concave_regions_chrom(d2y, chrom='.', tol=1e-10):
    s = np.where(np.diff((d2y < tol).astype(int))==1)[0] + 1
    e = np.where(np.diff((d2y < tol).astype(int))==-1)[0] + 1
    if s[0] > e[0]:
        s = np.insert(s, 0, 0)
    if s[-1] > e[-1]:
        e = np.insert(e, len(e), len(d2y))
    v = [-np.mean(d2y[i:j]) for i,j in zip(s,e)]
    m = [i + nanargmax_median(-d2y[i:j]) for i,j in zip(s,e)]

    df_regions = pd.DataFrame(collections.OrderedDict([
        ('chrom', [chrom]*len(s)),
        ('concave_start', s),
        ('concave_end', e),
        #('val', v),
        ('mode', m),
    ]))

    print('%d raw concave regions on chrom %s' % (len(df_regions), chrom))
    return df_regions

def find_concave_regions(fp_inp, fp_out_tsv, fp_out_bed):
    fh_inp = pyBigWig.open(fp_inp) 
    df_regions = pd.concat([
        find_concave_regions_chrom(d2y=fh_inp.values(chrom, 0, size, numpy=True), chrom=chrom)
        for chrom, size in itertools.islice(fh_inp.chroms().items(), None)], axis=0, ignore_index=True)
    fh_inp.close()
    print('%d peaks total' % (len(df_regions),))

    """
    write_gffbed(fp_out_bed,
        chrom = df_regions['chrom'],
        start = df_regions['concave_start'],
        end = df_regions['concave_end'],
        thickStart = df_regions['mode'],
        thickEnd = df_regions['mode'] + 1,
    )
    """

    return df_regions

def read_regions(fp, chroms, starts, ends, v=False):
    assert os.path.isfile(fp)
    n_clip = 0
    fh = pyBigWig.open(fp)
    for chrom, start_, end_ in zip(chroms, starts, ends):
        # fixes pyBigWig -- RuntimeError: You must supply a chromosome, start and end position.
        start = int(start_)
        end = int(end_)

        # Clip region if necessary
        if (0 <= start) and (end < fh.chroms(chrom)):
            r = fh.values(chrom, start, end)#, numpy=True)
        else:
            r = np.zeros(end - start) # Should get the partial signal
            n_clip += 1

        yield np.array(r)
    fh.close()
    if v: print(n_clip, 'clipped regions')

#df = pd.read_csv(input[0], sep='\t', names=['chrom', 'start', 'end', 'name', 'score', 'strand'])
#        df['score'] = list(map(lambda x: -x, read_regions(input[1], df.chrom, map(int, df.start), map(int, df.end), np.mean)))
#        #df['score'] = list(map(lambda x: -x, read_regions(input[1], df.chrom, map(int, df.start), map(int, df.end), np.min)))
#        df.to_csv(output[0], header=False, index=False, sep='\t')
def score(fp_inp, chroms, starts, ends, kernel):
    fh_inp = pyBigWig.open(fp_inp)
    d_chrom_d2smooth = {}
    for chrom, size in itertools.islice(fh_inp.chroms().items(), None):
        #print(chrom)
        val = fh_inp.values(chrom, 0, size, numpy=True)
        val_d2 = np.convolve(val, kernel, mode='same')
        d_chrom_d2smooth[chrom] = val_d2

    for chrom, start, end in zip(chroms, starts, ends):
        yield -np.mean(d_chrom_d2smooth[chrom][start:end])

def run_idr(chroms, starts, ends, scores_rep1, scores_rep2, prefix_out, condition):
    fp_rep1 = prefix_out + '_peaksidr_%(condition)s_rep1.bed' % locals()
    fp_rep2 = prefix_out + '_peaksidr_%(condition)s_rep2.bed' % locals()
    fp_iout = prefix_out + '_peaksidr_%(condition)s.bed' % locals()

    df_ = pd.DataFrame()
    df_['chrom'] = chroms
    df_['start'] = starts
    df_['end'] = ends
    df_['name'] = '.'
    df_['score_rep1'] = scores_rep1.clip(lower=0)
    df_['score_rep2'] = scores_rep2.clip(lower=0)
    df_['strand'] = '.'

    #https://github.com/nboley/idr/blob/74665e73bffb689a440948640c386b1188eea1e3/idr/idr.py#L64
    df_['col6'] = 0#float('nan') # signalValue
    df_['col7'] = 0#float('nan') # pValue
    df_['col8'] = 0#float('nan') # qValue

    df_[['chrom', 'start', 'end', 'name', 'score_rep1', 'strand', 'col6', 'col7', 'col8']].to_csv(fp_rep1, header=False, index=False, sep='\t')
    df_[['chrom', 'start', 'end', 'name', 'score_rep2', 'strand', 'col6', 'col7', 'col8']].to_csv(fp_rep2, header=False, index=False, sep='\t')

    idr_args = '--input-file-type bed --rank score --peak-merge-method max'
    idr_cmd = 'idr %(idr_args)s --samples %(fp_rep1)s %(fp_rep2)s --output-file %(fp_iout)s' % locals()
    print('Running idr on condition %(condition)s:' % locals())
    print(idr_cmd)
    os.system(idr_cmd)

def main(df_samples, prefix_out, kernel, min_concave_region_width, truncate_idr_input, fixed_peak_halfwidth):
    # fp_coverage
    fp_coverage = prefix_out + '_coverage.bw'
    if not(os.path.isfile(fp_coverage)):
        coverage(df_samples['fp'].tolist(), fp_coverage)
    else:
        assert is_bw(fp_coverage)
        print('Recycling existing coverage track: %s' % (fp_coverage,))

    # fp_d2smooth
    fp_d2smooth = prefix_out + '_d2smooth.bw'
    if not(os.path.isfile(fp_d2smooth)):
        print('Calculating d2smooth: %s' % (fp_d2smooth,))
        d2smooth(fp_coverage, fp_d2smooth, kernel)
    else:
        assert is_bw(fp_d2smooth)
        print('Recycling existing d2smooth track: %s' % (fp_d2smooth,))

    # fp_peaksall -- all raw peaks, scored by D2
    fp_peaksall_tsv = prefix_out + '_peaksall.tsv'
    fp_peaksall_bed = prefix_out + '_peaksall.bed'
    if not(os.path.isfile(fp_peaksall_tsv)):
        # call concave regions
        df_regions = find_concave_regions(fp_d2smooth, fp_peaksall_tsv, fp_peaksall_bed)

        # score concave regions
        for i, r in itertools.islice(df_samples.iterrows(), None):
            fp_ = r['fp']
            col_ = r['sample'] + '_score'
            print('scoring peaks for sample: %s' % (r['sample'],))
            df_regions[col_] = [*score(fp_, df_regions.chrom.tolist(), df_regions.concave_start.tolist(), df_regions.concave_end.tolist(), kernel)]

        # write all concave regions to output
        df_regions.to_csv(fp_peaksall_tsv, header=True, index=False, sep='\t')
        write_gffbed(fp_peaksall_bed,
            chrom = df_regions['chrom'],
            start = df_regions['concave_start'],
            end = df_regions['concave_end'],
            attr = df_regions[[sample + '_score' for sample in df_samples['sample']]],
            thickStart = df_regions['mode'],
            thickEnd = df_regions['mode'] + 1,
        )

    else:
        df_regions = pd.read_csv(fp_peaksall_tsv, sep='\t')
        print('Recycling existing scored concave regions (n=%d): %s' % (len(df_regions), fp_peaksall_tsv,))

    df_regions = df_regions.query('(concave_end - concave_start) >= @min_concave_region_width').reset_index(drop=True)
    print('%d peaks after discarding narrow concave regions (min_concave_region_width=%d)' % (len(df_regions), min_concave_region_width))

    # rank scores & truncate peak list based on "best" rank => 100,000 peaks
    #fp_peaksidr_wt_emb -- IDR peaks, scored by IDR
    #print(df_regions[[sample + '_score' for sample in df['sample']]].head(20))
    #print(df_regions[[sample + '_score' for sample in df['sample']]].head(20).rank(axis=0, ascending=False))
    print()
    print('%d raw peaks' % (len(df_regions),))
    df_regions['min_rank'] = df_regions[[sample + '_score' for sample in df_samples['sample']]].rank(axis=0, ascending=False).min(axis=1)
    df_regions_idr = df_regions.sort_values('min_rank').head(truncate_idr_input).sort_values(['chrom', 'concave_start', 'concave_end']).reset_index(drop=True)#.drop('min_rank', axis=1)
    
    print()
    print('curvature index-scores:')
    col_ = [sample + '_score' for sample in df_samples['sample']]
    print(df_regions[col_].head().to_string(line_width=10000, index=False))
    print()

    # run IDR using scores from each condition
    for condition in df_samples['condition'].unique():
        fp_idr = prefix_out + '_peaksidr_%(condition)s.bed' % locals()
        if not(os.path.isfile(fp_idr)):
            run_idr(df_regions_idr['chrom'], df_regions_idr['concave_start'], df_regions_idr['concave_end'], df_regions_idr['%(condition)s_rep1_score' % locals()], df_regions_idr['%(condition)s_rep2_score' % locals()], prefix_out, condition)
        else:
            print('Recycling existing IDR run: %s' % (fp_idr,))

        col_prefix = condition
        names_ = [
            'chrom', 'concave_start', 'concave_end', 'name',
            '%(col_prefix)s_scaledIDR' % locals(), 'strand',
            '%(col_prefix)s_localIDR' % locals(), '%(col_prefix)s_globalIDR' % locals(),
            '%(col_prefix)s_rep1_start' % locals(), '%(col_prefix)s_rep1_end' % locals(), '%(col_prefix)s_rep1_score' % locals(),
            '%(col_prefix)s_rep2_start' % locals(), '%(col_prefix)s_rep2_end' % locals(), '%(col_prefix)s_rep2_score' % locals(),
        ]
        df_ = pd.read_csv(fp_idr, names=names_, sep='\t').sort_values(['chrom', 'concave_start', 'concave_end']).reset_index(drop=True)
        df_regions_idr['%(col_prefix)s_globalIDR' % locals()] = df_['%(col_prefix)s_globalIDR' % locals()]

    print()
    print('globalIDR:')
    col_ = ['%(col_prefix)s_globalIDR' % locals() for col_prefix in df_samples['condition'].unique()]
    print(df_regions_idr[col_].head().to_string(line_width=10000, index=False))
    print()

    # Final peak boundaries -- fixed at flank_len bases either side of the mode
    if fixed_peak_halfwidth is None:
        df_regions_idr['start'] = df_regions_idr['concave_start']
        df_regions_idr['end'] = df_regions_idr['concave_end']
    
    else:
        df_regions_idr['start'] = df_regions_idr['mode'] - fixed_peak_halfwidth
        df_regions_idr['end'] = df_regions_idr['mode'] + fixed_peak_halfwidth + 1

    # Write final output of all peaks
    fp_regions_idr = prefix_out + '.tsv'
    df_regions_idr.to_csv(fp_regions_idr, header=True, index=False, sep='\t')
    print('Wrote all %d IDR-scored peaks to %s' % (len(df_regions_idr), fp_regions_idr))

    # Write final .bed-files at a range of thresholds
    col_ = ['%(col_prefix)s_globalIDR' % locals() for col_prefix in df_samples['condition'].unique()]
    df_regions_idr['max_globalIDR'] = df_regions_idr[col_].max(axis=1)
    for th in [0.001, 0.005, 0.01, 0.05]:
        th_globalIDR = -math.log(th, 10)
        fp_regions_idr_th = prefix_out + '_%(th)s.bed' % locals()
        df_regions_idr_th = df_regions_idr.query('max_globalIDR > @th_globalIDR').reset_index(drop=True)
        write_gffbed(fp_regions_idr_th,
            chrom = df_regions_idr_th['chrom'],
            start = df_regions_idr_th['start'],
            end = df_regions_idr_th['end'],
            attr = df_regions_idr_th[col_],
        )
        print('Wrote %d peaks at IDR=%.3f to %s' % (len(df_regions_idr_th), th, fp_regions_idr_th))

def parse_input_files(l_inp):
    l_records = []
    l_conditions = l_inp[0::3]
    l_fp_rep1 = l_inp[1::3]
    l_fp_rep2 = l_inp[2::3]
    for (condition, fp_rep1, fp_rep2) in zip(l_conditions, l_fp_rep1, l_fp_rep2):
        l_records.append([condition, condition + '_rep1', fp_rep1])
        l_records.append([condition, condition + '_rep2', fp_rep2])
    df_samples = pd.DataFrame.from_records(l_records, columns=['condition', 'sample', 'fp'])
    df_samples['is_bw'] = df_samples['fp'].map(is_bw)
    assert df_samples['is_bw'].all(), 'All input files do not look like BigWigs...'
    return df_samples

def makedirsp(fp):
    try:
        os.makedirs(fp)
    except:
        if not(os.path.isdir(fp)):
            raise

if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument('OUTPUT_PREFIX', help='Prefix to use for all output files')
        parser.add_argument('CONDITION_REP1_REP2', nargs='*', help='Name of the condition, BigWig files of first and second replicates')
        parser.add_argument('--smoothing-window-width', help='Width of the smoothing window used for the second derivative track', type=int, default=150)
        parser.add_argument('--smoothing-times', help='Number of times smoothing is applied to the second derivative', type=int, default=3)
        parser.add_argument('--min-concave-region-width', help='Discard concave regions smaller than the threshold specified', type=int, default=75)
        parser.add_argument('--truncate-idr-input', help='Truncate IDR input to the number of peaks specified', type=int, default=100000)
        parser.add_argument('--fixed-peak-halfwidth', help='Set final peak coordinates to the specified number of base pairs on either side of the concave region mode', type=int)
        args = parser.parse_args()

        df_samples = parse_input_files(args.CONDITION_REP1_REP2)
        print('Input samples:')
        print(df_samples.to_string(line_width=10000, index=False))
        print()

        print('Output prefix: %s' % (args.OUTPUT_PREFIX,))

        # Make output directory + intermediate directories if necessary -- fails sometimes
        #prefix_out = sys.argv[-1]
        #makedirsp(os.path.split(prefix_out)[0])

        main(
            df_samples=df_samples,
            prefix_out=args.OUTPUT_PREFIX,
            kernel=prepare_second_derivative_kernel(args.smoothing_window_width, args.smoothing_times),
            min_concave_region_width=args.min_concave_region_width,
            fixed_peak_halfwidth=args.fixed_peak_halfwidth,
            truncate_idr_input=args.truncate_idr_input,
        )

    except KeyboardInterrupt:
        #logging.warning('Interrupted')
        sys.exit(1)
